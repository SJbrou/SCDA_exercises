---
author:
  - name: "Liz Chan"
    id: 1234567
  - name: "Maaike Lamberst"
    id: 2671939
  - name: "Niek Schroor"
    id: 2671939
  - name: "Stan Brouwer"
    id: 2671939
format: 
  html:
    code-fold: show
    width: full
---

# Assignment 1 {.unnumbered}

## Data selection

We selected the EU superstore sales data provided by [tableau](https://public.tableau.com/app/learn/sample-data)\
It has\~10k sales (of differing quantities), divided over \~1k unique product_names. The product's category and sub-category are also provided

Before getting started, lets clear the workspace and load any dependencies.

```{r setup, results='hide'}
# Clear workspace
rm(list = ls())

# Method for loading dependencies
install_and_load <- function(packages) {
  installed_packages <- rownames(installed.packages())                                # Check installed packages
  to_install <- packages[!(packages %in% installed_packages)]
  if (length(to_install) > 0) {install.packages(to_install, dependencies = TRUE)}     # Install missing packages
  suppressMessages(lapply(packages, require, character.only = TRUE, quietly = TRUE))} # Load packages
install_and_load(c("tidyverse", "readxl", "ggplot2", "plotly", "dplyr", "lubridate", "gt", "stats")) # Method call with list of package names
```

## Data Pre-processing:

Lets load and transform the data correctly so it can be used

Lets start by loading the data, converting dates to a machine-readable format and checking for missing values:

```{r data_pre+processing, warining=FALSE, message=FALSE}
# Load the data
suppressWarnings({data <- read_excel("data/assignment1/sample_-_superstore.xls")})
# Note here that loading the data itself can give some warnings, which we supress. Probably due to excel having weird date-time formats that always cause frictions. 

# Ensure machine-readable date-time format
data$`Order Date` <- as.Date(data$`Order Date`, format = "%Y-%m-%d")

# Check for missing values in each column
missing_values <- colSums(is.na(data))
missing_values # Note, there are none.

# table of first 5 rows
data %>%
  head(5) %>%
  gt() %>%
  tab_header(
    title = "Superstore Sample - First 5 Rows"
  ) %>%
  cols_align(
    align = "center"
  ) %>%
  fmt_date(
    columns = vars(`Order Date`),
    date_style = 3  # date format as %month %d, $yyyy
  )


```

this looks great. Lets start visualization so we can see outliers, and decide if more data cleaning is required.

## Data visualization

Lets start by aggregating the sales per product category per month and plot the top 10 most sold products

```{r data_visualization, warning=FALSE, message=FALSE}
# Sum of Quantity for top products
top_products <- data %>%
  group_by(`Product Name`) %>%
  summarize(total_quantity = sum(Quantity, na.rm = TRUE)) %>%
  arrange(desc(total_quantity)) %>%
  top_n(20, total_quantity) %>%  # Top x products
  mutate(ProdName8 = substr(`Product Name`, 1, 8)) # Product names can be quite long and mess up layouts. Lets only plot the first 8 chars. 

# Plot with ggplot and ggplotly
plot <- ggplot(top_products, aes(x = reorder(ProdName8, -total_quantity), y = total_quantity)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Top 20 Most Sold Products",
       x = "Product ID",
       y = "Total Quantity") +
  theme_minimal() +
  coord_flip()

# Make the plot interactive
ggplotly(plot)

```

```{r}
# Aggregate quantity by Product Name and Order Date to create a time series
time_series_data <- data %>%
  group_by(`Product Name`, `Order Date`) %>%
  summarize(total_quantity = sum(Quantity, na.rm = TRUE)) %>%
  ungroup()

# Filter for the top products by total quantity sold (adjust as needed)
top_products <- time_series_data %>%
  group_by(`Product Name`) %>%
  summarize(total_quantity = sum(total_quantity)) %>%
  arrange(desc(total_quantity)) %>%
  slice_head(n = 10)  # Select top 10 products

# Filter the time-series data for only these top products
filtered_time_series_data <- time_series_data %>%
  filter(`Product Name` %in% top_products$`Product Name`) %>%
  mutate(ProdName8 = substr(`Product Name`, 1, 8)) # Product names can be quite long and mess up layouts. Lets only plot the first 8 chars. 


# Plot using the truncated product name
plot <- ggplot(filtered_time_series_data, aes(x = `Order Date`, y = total_quantity, color = ProdName8)) +
  geom_line(size = 1) +
  labs(title = "Quantity Sold Over Time per Product",
       x = "Order Date",
       y = "Quantity Sold") +
  theme_minimal() +
  theme(legend.position = "bottom") +
  scale_color_discrete(name = "Product Name")

# Make the plot interactive
ggplotly(plot)
```

Lets also plot the frequency of 20 most sold items

```{r descriptive_statistics2}
# Count frequency of top 20 products
top_products <- data %>%
  count(`Product Name`, sort = TRUE) %>%
  top_n(20, n) %>%
  mutate(ProdName8 = substr(`Product Name`, 1, 8))

# Plot!
ggplotly(ggplot(top_products, aes(x = reorder(`ProdName8`, -n), y = n)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Top 20 Most Sold Products",
       x = "Product Name",
       y = "Frequency") +
  theme_minimal() +
  coord_flip())
```

Unfortunately, these sales levels are too little to do meaningfull forecasting with. However, we can work with the sales aggregated on sub-category level. Lets visualise that data

```{r AggregateStatistics}

# Count frequency of top 20 products
top_categories <- data %>%
  count(`Sub-Category`, sort = TRUE)

# Plot!
ggplotly(ggplot(top_categories, aes(x = reorder(`Sub-Category`, -n), y = n)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Sub-Categories sorted",
       x = "Product Name",
       y = "Frequency") +
  theme_minimal() +
  coord_flip())
```

Also, lets include time-series for the top 10 sold sub-categories:

```{r AggregateStatistics2}
# Find top 3 most sold product names
top_3_categories <- top_categories$`Sub-Category`[0:9]

# Filter the data for  top 3 products
top_3_data <- data %>% filter(`Sub-Category` %in% top_3_categories)

# calculate sales per month
top_3_data <- top_3_data %>%
  mutate(Month = floor_date(`Order Date`, unit = "month"))

# Aggregate data by month for each product
top_3_data_aggregated <- top_3_data %>%
  group_by(Month, `Sub-Category`) %>%
  summarise(Sales_Count = n(), .groups = 'drop')

# Plot!
ggplotly(
  ggplot(top_3_data_aggregated, aes(x = Month, y = Sales_Count, color = `Sub-Category`, group = `Sub-Category`)) +
    geom_line(size = 1) +
    geom_point(size = 2) +  # Optional: adds points to make each data point clearer
    labs(title = "Monthly Sales for the Top 3 Most Sold Products",
         x = "Month",
         y = "Sales Count",
         color = "Product Name") +
    theme_minimal()
)


```

::: Column-margin
Should there be any more visualizations?\
Haven't looked at outliers / NA values etc\
(because I quickly assumed high-quality data due to source)

o Data visualization is essential for understanding the data before applying any forecasting methods. Using different tools, plots and charts, you will explore trends, seasonality, and any unusual patterns within your time series dataset.
:::

## Forecasting 

*4A -- Consider a part of your dataset including 3 products/services: For each of these 3 product/service, apply at least 3 relevant forecasting techniques/methods (please note that you need to justify why you applied these 3 methods among many others). For each product/service, evaluate these methods based on their accuracy (Note that the naïve, seasonal naïve, and moving average methods are not counted as these methods.).*

Lets forecast sales for the three most sold sub-categories:

```{r Forecasting}
# Find top 3 most sold product names
top_3_subcategories <- top_categories$`Sub-Category`[0:3]




# Filter the data for  top 3 products
top_3_data <- data %>% filter(`Sub-Category` %in% top_3_subcategories)

# calculate sales per month
top_3_data <- top_3_data %>%
  mutate(Month = floor_date(`Order Date`, unit = "month"))

# Aggregate data by month for each product
top_3_data_aggregated <- top_3_data %>%
  group_by(Month, `Sub-Category`) %>%
  summarise(Sales_Count = n(), .groups = 'drop')

# Create a time series object for each product
ts_data <- top_3_data_aggregated %>%
  pivot_wider(names_from = `Sub-Category`, values_from = Sales_Count, values_fill = 0) %>%
  select(-Month) %>%
  as.matrix()

# Create a time series object
ts_data <- ts(ts_data, start = c(2014, 1), end = c(2017, 12), frequency = 12)

# Forecasting
# ARIMA
# arima_forecasts <- lapply(1:ncol(ts_data), function(i) forecast(auto.arima(ts_data[, i]), h = 12))

# Exponential Smoothing
# ets_forecasts <- lapply(1:ncol(ts_data), function(i) forecast(ets(ts_data[, i]), h = 12))


# Plot the forecasts
# for (i in 1:3) {
#   plot(arima_forecasts[[i]], main = top_3_subcategories[i])
#   plot(ets_forecasts[[i]], main = top_3_subcategories[i])
# }

```